#!/bin/bash

# generate_hdfs_data
# author: Jonny Daenen
# Date Created: 2014-11-13
#
# Creates n rows, using m mappers and a python data generation function.
#
# hdfs_tmpdir will be created and removed after use!


genid=$(date +%s%N | cut -b1-13)


if [ $# -ne 8 ]
then
    echo "	Usage: generate_hdfs_data.sh num_rows num_chunks {function.py|builtin} funcname hdfs_tmpdir hdfs_outdir tabremoval{0|1} {hadoop|local}"
	exit 1
fi


#STREAMJAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar
# TODO auto-detect streaming jar
#STREAMJAR=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.5.2.jar
STREAMJAR=$HADOOP_STREAM_JAR

# store script location
#SCRIPTDIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
SCRIPTDIR=$( dirname $( readlink -f "${BASH_SOURCE[0]}" ) )
echo "Script dir:" $SCRIPTDIR


# prepare chunk indications
# -------------------------
# for a given number of tuples n and a number of splits m
# creates m files with intervals [0,n/m - 1], ... [n(m-1)/m, n - 1]

n=$1
m=$2
funcfile=$3
funcname=$4
inputdir=$5
outputdir=$6
REMOVETABS=$7 # TODO flag for tab removal
LOC=$8
ROWARG=$n

tmploc=$(mktemp -d)
parentindir=$inputdir
inputdir=$inputdir$tmploc
echo "changed input dir to" $inputdir

# determine data generator file
# -----------------------------

if [ "$funcfile" == "builtin" ];
then
	funcfile="functions.py"
	funcfilelocation=$SCRIPTDIR/$funcfile
else
	funcfilelocation=$funcfile
	funcfile=`basename $funcfile`
fi



#echo "function file: " $funcfilelocation


# create chunck indicators
# ------------------------

# for each of the m chunks
for i in $(seq 1 $m)
do
	echo "creating chunk " $i "..."
	
	# calculate bounds for current chunk
	let i=i-1
	let start=$i*$n/$m
	let end=($i+1)*$n/$m-1
	
	# create file
	filename=CHUNK_$i.txt
	echo $start $end $i > $tmploc/$filename
	
done



# execute data generator
# ----------------------

if [ "$LOC" == "local" ];
then
	cat ${tmploc}/CHUNK_*.txt | python $SCRIPTDIR/mapper.py $funcfilelocation $funcname $ROWARG
	
	if [ "$m" == "auto" ];
	then
		echo "auto-chunking enabled"
	fi
	# generate test data
	samplefile=$tmploc/sample.txt
	cat ${tmploc}/CHUNK_*.txt | python $SCRIPTDIR/mapper.py $funcfilelocation $funcname 1000 > $samplefile
	bytes=$(stat --printf="%s" $samplefile)
	
else
	
	echo "copying chunks to hdfs in folder " $inputdir

	# copy chunks to hdfs
	hdfs dfs -mkdir -p "$inputdir"
	hdfs dfs -copyFromLocal ${tmploc}/CHUNK_*.txt "$inputdir"


	mapcommand="mapper.py $funcfile $funcname $ROWARG"
	# the output format is set to make sure that the first tab is ignored
	# this is implemented to avoid extra tabs when no-tab output is generated
	outputformat="-outputformat mapreduce.extensions.NoTabOutputFormat"
	if [ "$REMOVETABS" == "0" ];
	then
		outputformat=""
		echo "tab removal disabled"
	else
		echo "tab removal active"
	fi

	echo $outputformat

	
	# note that the -files option needs to be before the normal options
	cmd="hadoop jar $STREAMJAR \
	  -D mapreduce.job.reduces=0 \
	  -libjars $SCRIPTDIR/NoTabFix.jar \
	  -input $inputdir \
	  -output $outputdir \
	  $outputformat \
	  -mapper \"$mapcommand\" \
	  -file $SCRIPTDIR/mapper.py \
	  -file $funcfilelocation"
	
	echo "$cmd"
	eval $cmd


	

	# -files $SCRIPTDIR/mapper.py,$funcfilelocation \
	# -D mapred.textoutputformat.separator=','\
	# -D stream.map.output.field.separator=',' \
	# -D stream.num.map.output.key.fields=0 \

	# remove data chunks on hdfs
	hdfs dfs -rm -r -skipTrash "$inputdir"
	hdfs dfs -rmdir "$parentindir/tmp"
	hdfs dfs -rmdir "$parentindir"
fi


# remove local chunk files
rm ${tmploc}/CHUNK_*.txt


echo "done."

